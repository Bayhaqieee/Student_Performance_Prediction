# -*- coding: utf-8 -*-
"""Students_Performance_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OaOZG2dNOt_52pqoUsMDeBl99fOX-0GO
"""

!pip install ucimlrepo
!pip install optuna

"""# Student Performance Prediction

- Author  : Muhammad Aditya Bayhaqie
- Assignment : Machine Learning Terapan (Dicoding)

### Data and Library Loading

Memuat Pustaka
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
import optuna
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

"""Mengimpor Dataset"""

from ucimlrepo import fetch_ucirepo

# fetch dataset
predict_students_dropout_and_academic_success = fetch_ucirepo(id=697)

# data (as pandas dataframes)
X = predict_students_dropout_and_academic_success.data.features
y = predict_students_dropout_and_academic_success.data.targets

# metadata
print(predict_students_dropout_and_academic_success.metadata)

"""### Data Assessment"""

# variable information
display(predict_students_dropout_and_academic_success.variables)
# After the information displayed, it is advised if you see it on table view if you use google collab

"""**Dari deskripsi data yang tersedia, dapat disimpulkan bahwa hanya terdapat lima fitur dengan tipe data kontinu yang dapat dianalisis lebih lanjut, khususnya untuk keperluan deteksi outlier.**

Namun demikian, fitur-fitur seperti **`Inflation rate`**, **`GDP`**, dan **`Unemployment Rate`** tidak akan disertakan dalam proses analisis dan pemodelan karena dianggap **tidak memiliki keterkaitan langsung dengan prediksi performa siswa**. Ketiga fitur tersebut merepresentasikan kondisi makroekonomi yang cakupannya lebih luas dan tidak secara spesifik memengaruhi variabel target dalam konteks ini.

#### Identifikasi Dataset
"""

# load the dataset
url = 'https://archive.ics.uci.edu/static/public/697/data.csv'
students = pd.read_csv(url)
students

"""**1. Jumlah Baris dan Kolom**
Dataset ini terdiri dari:
- **4.424 baris** (data mahasiswa)
- **37 kolom** (fitur)

---

**2. Uraian Fitur dalam Dataset**

| Fitur | Deskripsi |
|-------|-----------|
| `Marital Status` | Status pernikahan mahasiswa (1 = belum menikah, 2 = menikah, dst.) |
| `Application mode` | Metode/jenis pendaftaran yang digunakan oleh mahasiswa |
| `Application order` | Urutan pilihan program studi saat mendaftar |
| `Course` | ID program studi yang dipilih |
| `Daytime/evening attendance` | Jenis kelas yang diikuti (1 = kelas pagi, 0 = kelas malam) |
| `Previous qualification` | Kualifikasi akademik sebelumnya |
| `Previous qualification (grade)` | Nilai dari pendidikan sebelumnya |
| `Nacionality` | Kode negara kewarganegaraan |
| `Mother's qualification` | Tingkat pendidikan ibu |
| `Father's qualification` | Tingkat pendidikan ayah |
| `Mother's occupation` | Jenis pekerjaan ibu |
| `Father's occupation` | Jenis pekerjaan ayah |
| `Admission grade` | Nilai saat masuk universitas |
| `Displaced` | Apakah mahasiswa berasal dari wilayah terdampak atau pindahan (1 = ya, 0 = tidak) |
| `Educational special needs` | Apakah memiliki kebutuhan khusus (1 = ya, 0 = tidak) |
| `Debtor` | Apakah mahasiswa memiliki utang (1 = ya, 0 = tidak) |
| `Tuition fees up to date` | Status pembayaran uang kuliah (1 = ya, 0 = tidak) |
| `Gender` | Jenis kelamin (1 = laki-laki, 2 = perempuan) |
| `Scholarship holder` | Apakah mahasiswa penerima beasiswa (1 = ya, 0 = tidak) |
| `Age at enrollment` | Usia saat mendaftar kuliah |
| `International` | Apakah mahasiswa internasional (1 = ya, 0 = tidak) |
| `Curricular units 1st sem (credited)` | Jumlah SKS yang diakui di semester 1 |
| `Curricular units 1st sem (enrolled)` | Jumlah mata kuliah yang diambil di semester 1 |
| `Curricular units 1st sem (evaluations)` | Jumlah evaluasi yang diikuti di semester 1 |
| `Curricular units 1st sem (approved)` | Jumlah mata kuliah yang lulus di semester 1 |
| `Curricular units 1st sem (grade)` | Rata-rata nilai di semester 1 |
| `Curricular units 1st sem (without evaluations)` | Jumlah mata kuliah yang tidak diikuti evaluasinya |
| `Curricular units 2nd sem (credited)` | Jumlah SKS yang diakui di semester 2 |
| `Curricular units 2nd sem (enrolled)` | Jumlah mata kuliah yang diambil di semester 2 |
| `Curricular units 2nd sem (evaluations)` | Jumlah evaluasi yang diikuti di semester 2 |
| `Curricular units 2nd sem (approved)` | Jumlah mata kuliah yang lulus di semester 2 |
| `Curricular units 2nd sem (grade)` | Rata-rata nilai di semester 2 |
| `Curricular units 2nd sem (without evaluations)` | Jumlah mata kuliah semester 2 tanpa evaluasi |
| `Unemployment rate` | Tingkat pengangguran di tahun tersebut |
| `Inflation rate` | Tingkat inflasi ekonomi pada tahun tersebut |
| `GDP` | Pertumbuhan Produk Domestik Bruto saat itu |
| `Target` | Status akhir mahasiswa (`Dropout`, `Graduate`, `Enrolled`) |

---

**3. Kondisi Dataset**

- **Distribusi Tidak Seimbang (Imbalanced Classes):**
  - Fitur `Target` kemungkinan besar tidak seimbang antar kelas (Dropout, Graduate, Enrolled). Ini bisa mempengaruhi performa model klasifikasi.

#### Cek Missing Values
"""

null_counts = students.isnull().sum()

# Print the null counts
display(null_counts)

# Total number of null values in the DataFrame
total_nulls = null_counts.sum()
print(f"\nTotal number of null values: {total_nulls}")

"""Data yang telah dimuat berada dalam kondisi yang bersih. Tahapan selanjutnya adalah melakukan *Exploratory Data Analysis* (EDA) guna memahami karakteristik data secara lebih mendalam.

### Exploratory Data Analysis
"""

students.info()

"""Data telah berada dalam kondisi yang baik, dengan format setiap fitur yang sudah sesuai dan siap untuk dianalisis lebih lanjut."""

students.describe()

"""#### Identifikasi Outlier  
Langkah selanjutnya adalah mengidentifikasi **outlier** pada fitur-fitur kontinu yang telah ditentukan sebelumnya berdasarkan hasil deskripsi data. Proses ini bertujuan untuk memastikan tidak terdapat nilai ekstrem yang dapat memberikan pengaruh signifikan terhadap hasil analisis atau pemodelan.
"""

specific_cols = ['Previous qualification (grade)', 'Admission grade']
for col in specific_cols:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=students[col])  # Using students[col] to access data
    plt.title(f'Boxplot of {col}')
    plt.show()

"""Terdapat sejumlah outlier pada data kontinu yang telah dianalisis. Oleh karena itu, data yang terindikasi sebagai outlier akan dihapus dengan menggunakan parameter yang telah ditentukan, guna menjaga kualitas dan akurasi hasil analisis selanjutnya."""

# Specify the columns to check for outliers
cols_to_check = ['Previous qualification (grade)', 'Admission grade']

# Calculate quantiles and IQR for the specified columns only
Q1 = students[cols_to_check].quantile(0.25)
Q3 = students[cols_to_check].quantile(0.75)
IQR = Q3 - Q1

# Filter the dataset to remove outliers in the specified columns
filtered_students = students[~((students[cols_to_check] < (Q1 - 1.5 * IQR)) | (students[cols_to_check] > (Q3 + 1.5 * IQR))).any(axis=1)]

# Print the shape of the filtered dataset
print(filtered_students.shape)

"""#### Univariate Analysis
Analisis dilakukan secara terpisah berdasarkan jenis data, yaitu data kategorikal dan data numerikal. Langkah ini bertujuan untuk memahami distribusi dan karakteristik masing-masing fitur secara individu sebelum dilakukan analisis lebih lanjut.
"""

categorical_features = students.select_dtypes(include='object').columns.tolist()

numerical_features   = students.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_features   = [col for col in numerical_features if col != 'Id' and col != 'SalePrice']

discrete_features    = [col for col in numerical_features if len(students[col].unique()) < 25]
continuous_feature  = [col for col in numerical_features if col not in discrete_features]

print(f'Number of Categorical Feature : {len(categorical_features)}')
print(f'Number of Numerical Feature   : {len(numerical_features)}')
print(f'Number of Discrete Feature    : {len(discrete_features)}')
print(f'Number of Continous Feature   : {len(continuous_feature)}')

display(continuous_feature)

"""Data pada fitur kontinu merupakan hasil dari proses encoding sebelumnya. Fitur kontinu asli yang digunakan sejak awal adalah **`Previous qualification (grade)`** dan **`Admission grade`**, di mana data outlier pada kedua fitur tersebut telah dihapus pada tahap sebelumnya.

---

**Analisis Fitur Kategorikal**  
Selanjutnya, dilakukan analisis terhadap fitur-fitur kategorikal untuk memahami distribusi nilai dan potensi insight yang dapat diperoleh dari masing-masing kategori.

##### Categorical Features
"""

feature = categorical_features[0]
count = students[feature].value_counts()
percent = 100*students[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah Sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Data menunjukkan bahwa mayoritas siswa berada pada kategori `Graduate`, yang mengindikasikan tingkat kelulusan yang relatif tinggi dalam populasi tersebut. Hal ini mencerminkan keberhasilan akademik yang dominan di antara siswa dalam dataset.

##### Fitur Numerik  
Selanjutnya, dilakukan analisis terhadap fitur-fitur numerik untuk memahami distribusi, kecenderungan pusat, serta potensi nilai ekstrem yang mungkin memengaruhi hasil analisis secara keseluruhan.
"""

students.hist(bins=50, figsize=(20,15))
plt.show()

skewness = students[numerical_features].skew().sort_values(ascending=False)

avg_skewness = skewness
avg_skewness = avg_skewness.sort_values(ascending=False)

print(avg_skewness)

"""Informasi yang dapat diperoleh dari analisis awal adalah sebagai berikut:  
- Fitur-fitur kontinu cenderung memiliki distribusi *middle-skewed* atau sedikit miring ke satu sisi.  
- Persebaran data pada fitur lainnya bersifat cukup acak, hal ini disebabkan oleh sebagian besar data numerik yang tersedia merupakan hasil *encoding* dari variabel kategorikal.

#### Multivariate Analysis  
Analisis multivariat dilakukan untuk mengidentifikasi hubungan atau keterkaitan antara fitur-fitur dalam dataset.

##### Categorical Features  
Karena hanya terdapat satu fitur kategorikal, analisis akan difokuskan pada fitur-fitur numerik.

##### Numerical Features  
Selanjutnya, dilakukan analisis keterkaitan antar fitur numerik yang terdapat dalam data.
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(students, diag_kind = 'kde')

"""
##### Visualisasi Korelasi  
Analisis dilanjutkan dengan pemanfaatan visualisasi berupa _heatmap_ untuk menggambarkan korelasi antar fitur numerik secara menyeluruh."""

plt.figure(figsize=(20, 16))
correlation_matrix = students[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""#### Kesimpulan Hasil Analisis Korelasi  
Berdasarkan hasil dari dua visualisasi yang digunakan, dapat disimpulkan beberapa hal berikut:  
- Fitur dengan awalan `Curricular units 1st` dan `Curricular units 2nd` menunjukkan tingkat korelasi yang tinggi.  
- Fitur-fitur dengan korelasi rendah terhadap fitur lainnya antara lain:
  - Marital Status  
  - Nationality  
  - Displaced  
  - Father's Occupation  
  - Mother's Occupation  
  - International  
  - Unemployment Rate  
  - Inflation Rate  
  - GDP  

  Fitur-fitur tersebut akan dipertimbangkan untuk dihapus karena kontribusi informasinya yang rendah.

- Terdapat beberapa fitur yang memiliki korelasi tinggi satu sama lain, di antaranya:
  - Previous Qualification dan Admission Grade  
  - Age at Enrollment dan Scholarship Holder  

  Untuk fitur-fitur ini, akan dipertimbangkan untuk menghapus salah satu atau melakukan reduksi dimensi guna menghindari redundansi informasi.
"""

# Drop specified features
features_to_drop = ['Marital status', 'Nationality', 'Displaced', "Father's occupation", "Mother's occupation", 'International', 'Unemployment rate', 'Inflation rate', 'GDP']
students = students.drop(columns=features_to_drop, errors='ignore')

# Show the first few rows of the modified DataFrame
display(students)

"""**Penghapusan Fitur Redundan**  
Untuk fitur `Previous Qualification` dan `Admission Grade`, salah satu akan dihapus karena memiliki korelasi yang tinggi. Dalam hal ini, fitur `Previous Qualification` dipilih untuk dihapus.
"""

# Drop specified features
features_to_drop = ['Previous qualification (grade)']
students = students.drop(columns=features_to_drop, errors='ignore')

# Show the first few rows of the modified DataFrame
display(students)

"""### Data Preparation

Pada tahap ini, dilakukan beberapa proses persiapan data untuk memastikan dataset siap digunakan dalam proses pemodelan machine learning. Berikut adalah langkah-langkah yang telah dilakukan secara keseluruhan:

---

**1. Handling Missing Values (Menangani Nilai Kosong)**

- Kolom - Kolom yang memiliki Missing Values yang besar akan kita drop

---

**2. Feature Encoding (Encoding Fitur Kategorikal)**

- Fitur kategorikal yang Telah diubah menggunakan:
  - **Label Encoding** untuk fitur ordinal atau biner.
  - **One-Hot Encoding** untuk fitur dengan banyak kategori tanpa urutan.

- Seluruh dataset telah di *Encode* sehingga tidak perlu lagi melakukan fase ini

---

**3. Outlier Handling (Menghapus Outlier)**

- Deteksi outlier dilakukan menggunakan:
  - **IQR (Interquartile Range)** dan
  - **Z-score Method** untuk kolom numerik seperti `Age at enrollment`, `grade`, dan `GDP`.
- Outlier ekstrem dihapus atau diganti jika memengaruhi distribusi secara signifikan.

---

**4. Dimensionality Reduction (Reduksi Dimensi)**

- **Principal Component Analysis (PCA)** dilakukan untuk mengurangi dimensi data tanpa kehilangan banyak informasi.
- PCA diterapkan setelah fitur numerik dinormalisasi untuk meningkatkan efisiensi proses training model dan menghindari multikolinearitas antar fitur.

---

**5. Train-Test Split (Pembagian Data)**

- Dataset dibagi menjadi:
  - **80% data latih**
  - **20% data uji**
- Pembagian dilakukan secara **stratified** berdasarkan label `Target` untuk menjaga distribusi kelas tetap seimbang di kedua subset.

#### Handling Missing Values
"""

null_counts = students.isnull().sum()

# Print the null counts
display(null_counts)

# Total number of null values in the DataFrame
total_nulls = null_counts.sum()
display(f"\nTotal number of null values: {total_nulls}")

"""Karena data telah bersih, maka tidak diperlukan lagi penanganan terhadap Missing Values

#### Feature Encoding

Karena data telah dikodekan, maka tidak diperlukan lagi eksekusi pada Feature Encoding

#### Reduksi Dimensi dengan Principal Component Analysis (PCA)  
Seperti yang telah dianalisis sebelumnya, fitur-fitur dengan awalan `Curricular units 1st` dan `Curricular units 2nd` akan direduksi menggunakan metode **Principal Component Analysis (PCA)** secara terpisah.

Curricular units 1st
"""

# Select columns for PCA
curricular_units_1st_cols = [col for col in students.columns if 'Curricular units 1st' in col]
curricular_units_2nd_cols = [col for col in students.columns if 'Curricular units 2nd' in col]

# Create a DataFrame with selected columns
curricular_units_data = students[curricular_units_1st_cols + curricular_units_2nd_cols]

# Visualize relationships using pairplot
sns.pairplot(curricular_units_data, plot_kws={"s": 3});

"""**Korelasi antar Fitur Kandidat PCA**  
Data menunjukkan adanya keterkaitan positif yang signifikan antar fitur-fitur yang akan direduksi, sehingga layak untuk diterapkan teknik reduksi dimensi.
"""

# Scale the data before applying PCA
scaler = StandardScaler()
curricular_units_scaled = scaler.fit_transform(curricular_units_data)

# Apply PCA with 3 components (adjust as needed)
pca = PCA(n_components=7, random_state=123)
pca.fit(curricular_units_scaled)
principal_components = pca.transform(curricular_units_scaled)

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_.round(2)
print("Explained Variance Ratio:", explained_variance)

"""**Hasil dan Langkah Reduksi Dimensi**  
Berdasarkan hasil PCA, tiga komponen utama pertama mampu menjelaskan hingga 90% dari total variansi data. Oleh karena itu, langkah-langkah yang akan dilakukan adalah sebagai berikut:

- Menggunakan `n_components = 3` untuk mengekstraksi tiga komponen utama.  
- Melatih model PCA dengan data masukan yang telah ditentukan.  
- Menambahkan fitur baru ke dalam dataset dengan nama `dimension` yang merupakan hasil dari proses transformasi.  
- Menghapus kolom-kolom dengan nama `Curricular units 1st` dan `Curricular units 2nd`.
"""

# Create new columns in the 'students' DataFrame for principal components
for i in range(3):
    students[f'Curricular_Units_PCA_{i+1}'] = principal_components[:, i]

# Drop the original Curricular units columns
students = students.drop(columns=curricular_units_1st_cols + curricular_units_2nd_cols)

display(students)

"""Untuk fitur `Age at Enrollment` dan `Scholarship Holder`, meskipun memiliki korelasi tinggi, keduanya memiliki makna yang berbeda secara konseptual. Oleh karena itu, keduanya tetap dipertahankan dalam dataset.

#### Pemisahan Data: Train-Test Split  
Proses pemisahan data dilakukan dengan rasio 80:20, di mana 80% data digunakan untuk pelatihan (training) dan 20% sisanya untuk pengujian (testing).
"""

X = students.drop(["Target"],axis =1)
y = students["Target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""#### Standarisasi Data"""

# Identify numerical features
numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns

# Scale numerical features using StandardScaler
scaler = StandardScaler()
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])
display(X_train[numerical_features])

X_train[numerical_features].describe().round(4)

"""### Pengembangan Model  

Pada tahap ini, dilakukan proses pengembangan dan pelatihan beberapa model *machine learning* untuk memprediksi status mahasiswa (Dropout atau Graduate) berdasarkan fitur-fitur yang tersedia dalam dataset. Model yang dibangun antara lain:

- Gradient Boosting (menggunakan **XGBoost**)
- **Logistic Regression**
- **Support Vector Machine (SVM)**
- **K-Nearest Neighbors (KNN)**
- **Random Forest**

---

**1. Gradient Boosting (XGBoost)**
*Cara kerja:*  
XGBoost adalah algoritma ensemble yang menggunakan teknik *boosting*, di mana model dibangun secara berurutan. Setiap model baru berusaha memperbaiki kesalahan dari model sebelumnya, dan semua model digabung untuk membuat prediksi akhir yang lebih akurat.

*Parameter utama yang digunakan:*
- `random_state` : 123

---

**2. Logistic Regression**
*Cara kerja:*  
Logistic Regression digunakan untuk memodelkan probabilitas dari kelas target biner (0 atau 1). Fungsi logistik mengubah output menjadi nilai probabilitas antara 0 dan 1.

*Parameter utama yang digunakan:*
- `max_iter` : 1000
- `random_state` : 123

---

**3. Support Vector Machine (SVM)**
*Cara kerja:*  
SVM bekerja dengan mencari hyperplane optimal yang memisahkan data dua kelas dengan margin maksimum. SVM efektif untuk ruang fitur berdimensi tinggi.

*Parameter utama yang digunakan:*
- `kernel` : "rbf"
- `random_state` : 123

---

**4. Random Forest**
*Cara kerja:*
Random Forest adalah metode ensemble berbasis *bagging* yang membangun banyak pohon keputusan dan menggabungkan hasilnya untuk meningkatkan akurasi dan mengurangi overfitting.

*Parameter utama yang digunakan:*
- `n_estimators` : 474
- `max_depth` : 20
- `random_state` : 55
- `n_jobs` : -1

---

Catatan:
Pada tahap ini **belum dilakukan hyperparameter tuning** lebih lanjut. Model-model di atas digunakan dengan parameter default atau sedikit modifikasi sederhana untuk eksplorasi awal performa.

---

Model yang telah dikembangkan ini akan dievaluasi menggunakan metrik seperti *accuracy*, *precision*, *recall*, dan *F1-score* untuk memilih model terbaik.

**Gradient Boosting (GBM)**  
Gradient Boosting bekerja dengan membangun model secara bertahap, di mana setiap model baru mencoba memperbaiki kesalahan model sebelumnya. Model ini menggunakan pendekatan boosting, yaitu menggabungkan beberapa pohon keputusan lemah (weak learners) menjadi model yang lebih kuat. GBM sangat efektif dalam menangani data tabular dan sering digunakan dalam kompetisi machine learning karena kemampuannya dalam menangkap pola yang kompleks.
"""

# Gradient Boosting (XGBoost alternative)
gb = GradientBoostingClassifier(random_state=123)
gb.fit(X_train, y_train)
gb_predictions = gb.predict(X_test)

"""**Logistic Regression**  
Logistic Regression adalah model statistik yang digunakan untuk memprediksi probabilitas suatu peristiwa berdasarkan variabel input. Model ini bekerja dengan menggunakan fungsi sigmoid untuk mengubah hasil regresi linear menjadi nilai probabilitas antara 0 dan 1. Logistic Regression cocok digunakan dalam kasus klasifikasi biner seperti memprediksi apakah seorang siswa akan lulus atau tidak.
"""

# Logistic Regression
logreg = LogisticRegression(max_iter=1000, random_state=123)
logreg.fit(X_train, y_train)
logreg_predictions = logreg.predict(X_test)

"""**Support Vector Machine (SVM)**  
SVM bekerja dengan mencari hyperplane terbaik yang dapat memisahkan kelas-kelas dalam data. Model ini menggunakan konsep margin maksimal, yaitu memilih hyperplane yang memiliki jarak terjauh dari titik-titik terdekat dari setiap kelas. Jika data tidak dapat dipisahkan secara linear, SVM dapat menggunakan kernel trick untuk mengubah data ke dimensi yang lebih tinggi agar lebih mudah dipisahkan. SVM sangat efektif dalam menangani data yang tidak terstruktur dan memiliki distribusi yang kompleks.
"""

# Support Vector Machine (SVM)
svm = SVC(kernel="rbf", random_state=123)
svm.fit(X_train, y_train)
svm_predictions = svm.predict(X_test)

"""**Random Forest**

Random forest merupakan salah satu model machine learning yang termasuk ke dalam kategori ensemble (group) learning. Apa itu model ensemble? Sederhananya, ia merupakan model prediksi yang terdiri dari beberapa model dan bekerja secara bersama-sama.
"""

# buat model prediksi
RF = RandomForestClassifier(n_estimators= 474, max_depth= 20, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)
RF_predictions = RF.predict(X_test)

"""### Evaluation

**Pra-pemrosesan Data**  
Sebelum proses pelatihan model, data akan dilakukan *scaling* terlebih dahulu untuk menghindari terjadinya kebocoran data dan memastikan setiap fitur berada pada skala yang sebanding.
"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

# Evaluate using classification metrics
# Buat variabel scores yang isinya adalah dataframe nilai accuracy, precision, recall, f1 score data train dan test pada masing-masing algoritma
scores = pd.DataFrame(columns=['train_accuracy', 'test_accuracy', 'train_precision', 'test_precision', 'train_recall', 'test_recall', 'train_f1', 'test_f1'], index=['GB','LG','SVM'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'GB': gb, 'LG': logreg, 'SVM': svm, 'RF': RF}

# Hitung metrik evaluasi masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    scores.loc[name, 'train_accuracy'] = accuracy_score(y_true=y_train, y_pred=model.predict(X_train))
    scores.loc[name, 'test_accuracy'] = accuracy_score(y_true=y_test, y_pred=model.predict(X_test))
    scores.loc[name, 'train_precision'] = precision_score(y_true=y_train, y_pred=model.predict(X_train), average='weighted')
    scores.loc[name, 'test_precision'] = precision_score(y_true=y_test, y_pred=model.predict(X_test), average='weighted')
    scores.loc[name, 'train_recall'] = recall_score(y_true=y_train, y_pred=model.predict(X_train), average='weighted')
    scores.loc[name, 'test_recall'] = recall_score(y_true=y_test, y_pred=model.predict(X_test), average='weighted')
    scores.loc[name, 'train_f1'] = f1_score(y_true=y_train, y_pred=model.predict(X_train), average='weighted')
    scores.loc[name, 'test_f1'] = f1_score(y_true=y_test, y_pred=model.predict(X_test), average='weighted')

# Print the scores
display(scores)

"""Model Random Forest menunjukkan kecenderungan *overfitting*. Oleh karena itu, perlu dilakukan evaluasi lebih lanjut terhadap performa masing-masing model untuk menentukan model yang paling optimal."""

# Print classification report for each model
for name, model in model_dict.items():
    print(f"Classification Report for {name}:")
    print(classification_report(y_test, model.predict(X_test)))

"""Berdasarkan metrik F1 Score, algoritma Gradient Boosting menunjukkan tingkat akurasi yang relatif lebih tinggi dibandingkan model lainnya. Namun demikian, akurasi yang dicapai masih belum cukup memuaskan untuk dikategorikan sebagai model yang optimal. Evaluasi dan penyempurnaan lebih lanjut tetap diperlukan."""

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({'Actual': y_test, 'GB_Prediction': gb_predictions,
                             'LogReg_Prediction': logreg_predictions, 'SVM_Prediction': svm_predictions, 'RF_Prediction': RF_predictions})

# Display the comparison table
comparison_df